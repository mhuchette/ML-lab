{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Spark ML\n",
    "\n",
    "### In this notebook, we will explore machine learning using Spark ML. We will exploit Spark ML's high-level APIs built on top of DataFrames to create and tune machine learning pipelines. Spark ML Pipelines enable combining multiple algorithms into a single pipeline or workflow. We will utilize Spark ML's feature transformers to convert, modify and scale the features that will be used to develop the machine learning model. Finally, we will evaluate and cross validate our model to demonstrate the process of determining a best fit model and load the results in the database.\n",
    "\n",
    "### We are using machine learning to try to predict diagnoses of breast cancer tumors. We will use data on tumors that have already been diagnosed as benign or malignant as a training set for the algorithm .Â¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents \n",
    "1. [Create Version](#version)\n",
    "2. [Import Libraries](#import)\n",
    "3. [Read from Cloud Object Storage](#read)\n",
    "4. [Transform data](#transform)\n",
    "5. [PixieDust](#pixie)\n",
    "6. [Feature Engineering](#engineer)\n",
    "7. [Define model](#model)\n",
    "8. [Create pipeline](#pipeline)\n",
    "9. [Train model](#train)\n",
    "10. [Evaluate accuracy](#evaluate)\n",
    "11. [Hyperparameter Tuning](#tuning)\n",
    "12. [Choose and Utilize the best-fit model](#best)\n",
    "13. [Save Results to Cloud Object Storage](#credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"version\"></a>\n",
    "## Create Version \n",
    "Save a version of the notebook by selecting <b>File</b> > <b>Save Version</b> \n",
    "<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/FileOptions.PNG\" > or by selecting the <b>Versions</b> icon and selecting <b>Save Version</b>. <img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/versions-button.png\" ><br>\n",
    "You can have up to ten (10) versions of a notebook.   Notebook versions are saved in a FIFO manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure Spark is installed and up to date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The spark version is {}.'.format(spark.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"import\"></a>\n",
    "## Import the required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports for Spark\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import NaiveBayes, DecisionTreeClassifier\n",
    "from pyspark.sql.functions import year\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Imports for pixiedust\n",
    "from pixiedust.display import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"read\"></a>\n",
    "## Read from Cloud Object Storage\n",
    "Click on the cell below, then on the notebook toolbar, click the box of 1's and 0's, which is the <b>Find and Add Data</b> icon.\n",
    "\n",
    "<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/bleonardb3/WatsonStudio/master/Lab-1/images/onezeroicon.png\" >\n",
    "\n",
    "Select the Files view then click on **Insert to code**, then click on **Insert SparkSession DataFrame**.\n",
    "\n",
    "Rename the dataframe to **cancer_df**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert SparkSession DataFrame here\n",
    "# make CERTAIN to rename the default dataframe name to cancer_df\n",
    "import ibmos2spark\n",
    "\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'api_key': '54eK5hQft9uVYbrpsxfWs4Tdgw8sbc8weZndKdj8iwhj',\n",
    "    'service_id': 'iam-ServiceId-e54eecd9-137d-44ca-b6d3-87b14b0ad21a',\n",
    "    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token'}\n",
    "\n",
    "configuration_name = 'os_af2c2a9305dd4cc6a1ce91fbe599e4bb_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "cancer_df = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('cancer_data.csv', 'mlproject-donotdelete-pr-rwqegunlryj5yq'))\n",
    "cancer_df.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"transform\"></a>\n",
    "## Identify labels and features to transform\n",
    "A label is the output of an ML model. Since we are creating a model to predict whether a tumor is malignant or benign our label will be **diagnosis**.  \n",
    "\n",
    "A feature is an input of the ML model, in this case, the columns that we are using to help predict the diagnosis of a tumor (area, smoothness, etc). We will need to tranform each feature into a double so that it can be used properly later on to create, train and test the model.\n",
    "\n",
    "Spark uses Resilient Distributed Datasets (RDDs) which are immutable, meaning if we need to transform a column of the dataframe we must create a new RDD each time we perform a transformation. We can do so using withColumn() and because each feature must be cast as a double we need to transform each feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "color": "diagnosis",
      "handlerId": "scatterPlot",
      "keyFields": "id",
      "rowCount": "1000",
      "table_showrows": "All",
      "valueFields": "area_mean"
     }
    }
   },
   "outputs": [],
   "source": [
    "df_cols = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\",\"compactness_mean\",\"concavity_mean\",\"concave points_mean\",\"symmetry_mean\",\"fractal_dimension_mean\",\"radius_se\",\"texture_se\",\"perimeter_se\",\"area_se\",\"smoothness_se\",\"compactness_se\",\"concavity_se\",\"concave points_se\",\"symmetry_se\",\"fractal_dimension_se\",\"radius_worst\",\"texture_worst\",\"perimeter_worst\",\"area_worst\",\"smoothness_worst\",\"compactness_worst\",\"concavity_worst\",\"concave points_worst\",\"symmetry_worst\",\"fractal_dimension_worst\"]\n",
    "\n",
    "dfprev = (cancer_df.withColumn(\"idTemp\", cancer_df[\"id\"]\n",
    "    .cast(\"Double\")).drop(\"id\").withColumnRenamed(\"idTemp\", \"id\"))\n",
    "\n",
    "for df_col in df_cols:\n",
    "    dfnew = (dfprev.withColumn(\"temp\", dfprev[df_col]\n",
    "             .cast(\"Double\")).drop(df_col).withColumnRenamed(\"temp\",df_col))\n",
    "    dfprev = dfnew\n",
    "    \n",
    "cancer_nums = dfprev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pixie\"></a>\n",
    "## Using PixieDust \n",
    "We can use PixieDust to explore the data. PixieDust is an open source tool to add visualizations to Jupyter Notebooks and simplifies the visual options of python.\n",
    "\n",
    "By clicking the grid icon in the output of the cell below we can see a table of the dataframe.\n",
    "\n",
    "If we click the drop down menu (next to the grid icon), symbolized with the graph, we can choose to put the data into a scatterplot. We can choose **id** as the key, one of the feature columns (**area_mean**, **perimeter_mean**, etc.) as the value and set the number of rows to display to 1000. We are given a scatterplot of our data points and by choosing the bokeh renderer on the right side and setting the color to diagnosis we can see the spread of the different diagnoses.\n",
    "\n",
    "Example: Set the key for the graph to **id**, the value to **area_mean**, and color to diagnosis. The plot provided allows us to infer that most benign tumors have smaller mean area when compared to malignant tumors. On the far right of the graph we are presented with our unlabeled points (green points) and using the trend of the labeled points we can predict which of the unlabeled tumors may be benign and malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "color": "diagnosis",
      "handlerId": "scatterPlot",
      "keyFields": "id",
      "rowCount": "1000",
      "valueFields": "area_mean"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(cancer_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use PixieDust to observe the composition of the data. \n",
    "Below we group the data by diagnosis so that we can get a count of the number of each diagnosis (M-malignant, B-benign, U-unlabeled). \n",
    "\n",
    "Using the table feature we can see that the data has 305 benign tumors, 193 malignant tumors, and 71 unlabeled tumors. \n",
    "\n",
    "We can also use a pie chart, created using the drop down menu used to make the scatterplot above, to see the percentages of each diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "handlerId": "pieChart",
      "keyFields": "diagnosis",
      "rowCount": "1000",
      "valueFields": "count"
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cancer = cancer_nums.groupby(\"diagnosis\").count()\n",
    "display(cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because some of our data points are unlabeled we need to filter them out before we train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabeledData = cancer_nums.filter(\"diagnosis != 'U'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"engineer\"></a>\n",
    "## Feature Engineering \n",
    "We need to change our label, **diagnosis**, into a form that SparkML can use. We can do so by using StringIndexer which converts textual data into numeric data while keeping the context categorical. It encodes the input column, in our case the diagnosis column, to a column of indices between 0 and the number of labels where the most frequent label has index 0.\n",
    "\n",
    "To turn the label back into a human readable form we can use the converter that we define below using IndexToString. IndexToString maps the column of indices back to the original labels of the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"engineer\"></a>\n",
    "## Feature Engineering \n",
    "We need to change our label, **diagnosis**, into a form that SparkML can use. We can do so by using StringIndexer which encodes the input column, in our case the diagnosis column, to a column of indices that are representative of the frequency of the input string.\n",
    "\n",
    "To turn the label back into a human readable form we can use the converter that we define below using IndexToString. IndexToString maps the column of indices back to the original labels of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"diagnosis\", outputCol=\"label\", handleInvalid=\"error\")\n",
    "labelModel = labelIndexer.fit(LabeledData)\n",
    "converter = IndexToString(inputCol=\"prediction\", outputCol=\"predCategory\", labels=labelModel.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must put all feature columns into an array using VectorAssembler which combines a list of columns into a single vector column that can be used in training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=[\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\",\"compactness_mean\",\"concavity_mean\",\"concave points_mean\",\"symmetry_mean\",\"fractal_dimension_mean\",\"radius_se\",\"texture_se\",\"perimeter_se\",\"area_se\",\"smoothness_se\",\"compactness_se\",\"concavity_se\",\"concave points_se\",\"symmetry_se\",\"fractal_dimension_se\",\"radius_worst\",\"texture_worst\",\"perimeter_worst\",\"area_worst\",\"smoothness_worst\",\"compactness_worst\",\"concavity_worst\",\"concave points_worst\",\"symmetry_worst\",\"fractal_dimension_worst\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a normalizer to normalize each vector to have a standard form to improve the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## State the model to be used \n",
    "There are many different types of models that can be used with Spark from NaÃ¯ve Bayes to Random Forest. For our dataset a Logistic Regression model is most appropriate since they are often used for models of binary categorical outcome. The model will output its prediction into the **prediction** column.\n",
    "\n",
    "For more information on the models that can be used with Spark and how to determine which model to use for your dataset see: https://spark.apache.org/docs/2.2.0/ml-classification-regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pipeline\"></a>\n",
    "## Create a pipeline\n",
    "A pipeline is a sequence of stages that are to be run in a specific order where each stage is either a transformer or an estimator. Transformers convert dataframes into other dataframes, usually by appending a column, while estimators, such as LogisticRegression, call fit() and train a Logistic Regression Model which is a transformer since it adds a **predictions** column. A pipeline chains together multiple tranformers to specify a specific ML workflow. \n",
    "\n",
    "In our case, we first want to tranform the label into a usable form for the algorithm, then we create a combined vector, normalize the vector, fit the model, and finally convert the results back into the original label format.\n",
    "\n",
    "For more information on Pipelines visit https://spark.apache.org/docs/latest/ml-pipeline.html#pipeline-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[labelIndexer, vecAssembler, normalizer, lr, converter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## Divide data into a training and testing set \n",
    "We should split the data randomly with 70% going to the training set and 30% going to the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = LabeledData.randomSplit([70.0,30.0], seed=1)\n",
    "train.cache()\n",
    "test.cache()\n",
    "print('The number of records in the training data set is {}.'.format(train.count()))\n",
    "print('The number of rows labeled M is {}.'.format(train.filter(train['diagnosis'] == \"M\").count()))\n",
    "print('The number of rows labeled B is {}.'.format(train.filter(train['diagnosis'] == \"B\").count()))\n",
    "\n",
    "print()\n",
    "\n",
    "print('The number of records in the test data set is {}.'.format(test.count()))\n",
    "print('The number of rows labeled M is {}.'.format(test.filter(train['diagnosis'] == \"M\").count()))\n",
    "print('The number of rows labeled B is {}.'.format(test.filter(train['diagnosis'] == \"B\").count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model and make predictions \n",
    "Next, we need to fit the pipeline to the model and make predictions using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)\n",
    "predictions = model.transform(test)\n",
    "predictions.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluate\"></a>\n",
    "## Evaluate the accuracy of the model \n",
    "We can use the Receiver Operator Characteristic (ROC) curve for binary classifiers which assess the models diagnositc ability. The ROC is useful for comparing models and choosing the best one for the data. An ROC value close to 1 performs very well, whereas a model with an ROC value close to 0.5 is as good as flipping a coin. The value is calculated by plotting the true positive rate (recall / probability of detection) against the false positive rate (fall-out / probability of a false alarm) at various levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setMetricName(\"areaUnderROC\")\n",
    "print('Area under the ROC curve = {}.'.format(evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tuning\"></a>\n",
    "## Hyperparameter Tuning\n",
    "To find the best possible model we can use model selection tools that:\n",
    "\n",
    "1. Split the data into training and testing datasets\n",
    "2. For each (training, test) pair, iterate through the set of ParamMaps\n",
    "3. For each ParamMap, fit the pipeline using those parameters, get a fitted Model, and evaluate the Modelâs performance\n",
    "4. Select the Model found using the best-performing set of parameters\n",
    "\n",
    "We must first specify which parameter values we would like the tool to test in order to find the best model. For our model we have the option to test different values for normalizer and/or any of the parameters of the Logistic Regression Model (maxIter, elasticNetParam, regParam, etc). \n",
    "\n",
    "One of the parameters being adjusted below is elasticNetParam, which must be between 0 and 1. If you would like your model to be closer to a Lasso regression and therefore set coefficients that are not relevant equal to 0, you can choose a value close to 1. If you'd like your model to be closer to a Ridge regression and therefore minimize the impact of irrelevant coefficients without setting them to 0 and regularize the model, you can choose a value close to 0.\n",
    "\n",
    "The second parameter in the paramGrid below is the normalizer, which is important because it ensures that the algorithm runs correctly. The value that you set for the normalizer represents the p-norm used for normalization. \n",
    "\n",
    "To tune our model we will use cross-validation.\n",
    "A CrossValidator splits the dataset into folds which are used as separate training and testing datasets. We will use 10 folds, so the CrossValidator will generate 10 dataset pairs for training and testing. It will use the average evaluation of the 10 models found using each dataset pair to identify the best ParamMaps and re-fit the pipeline using the best ParamMap for the entire dataset.\n",
    "\n",
    "When working with Spark you also have the option to do hyperparameter tuning using a train-validation split which evaluates each ParamMap once, as opposed to k times like cross-validaition. The benefit of train-validation is that it is less expensive but it can be much less reliable if the training dataset isn't sufficiently large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder().addGrid(lr.elasticNetParam, [0, 0.5, 1.0])\n",
    "                 .addGrid(normalizer.p, [1.0, 3.0]).build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(train)\n",
    "print('Area under the ROC curve for best fitted model = {}.'.format(evaluator.evaluate(cvModel.transform(test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cross-validated model was 0.58% better at predicting a diagnosis than our original model so we will use the new model for our final predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Area under the ROC curve for non-tuned model = {}.'.format(evaluator.evaluate(predictions)))\n",
    "print('Area under the ROC curve for best fitted model = {}.'.format(evaluator.evaluate(cvModel.transform(test))))\n",
    "print('Improvement = {0:0.2f}%'.format((evaluator.evaluate(cvModel.transform(test)) - evaluator.evaluate(predictions)) *100 / evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"best\"></a>\n",
    "## Finalizing and making predictions using the best model\n",
    "First we must filter the data to have only the points that are unlabeled so that we can make predictions on diagnoses for those points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnlabeledData=cancer_nums.filter(\"diagnosis == 'U'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPreds = cvModel.transform(UnlabeledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPreds.select(\"id\",\"prediction\",\"predCategory\",\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\",\"compactness_mean\",\"concavity_mean\",\"concave points_mean\",\"symmetry_mean\",\"fractal_dimension_mean\",\"radius_se\",\"texture_se\",\"perimeter_se\",\"area_se\",\"smoothness_se\",\"compactness_se\",\"concavity_se\",\"concave points_se\",\"symmetry_se\",\"fractal_dimension_se\",\"radius_worst\",\"texture_worst\",\"perimeter_worst\",\"area_worst\",\"smoothness_worst\",\"compactness_worst\",\"concavity_worst\",\"concave points_worst\",\"symmetry_worst\",\"fractal_dimension_worst\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the overall prediction count for diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The number of records in the unlabeled data set is {}.'.format(newPreds.count()))\n",
    "print('The number of rows labeled malignant is {}.'.format(newPreds.filter(newPreds['predCategory'] == \"M\").count()))\n",
    "print('The number of rows labeled benign is {}.'.format(newPreds.filter(newPreds['predCategory'] == \"B\").count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"credentials\"></a>\n",
    "## Insert the object storage credentials\n",
    "\n",
    "Click on the cell below, then on the notebook toolbar, click the box of 1's and 0's, which is the <b>Find and Add Data</b> icon.\n",
    "\n",
    "<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/bleonardb3/WatsonStudio/master/Lab-1/images/onezeroicon.png\" >\n",
    "\n",
    "Under the cancer_data.csv file, click <b>Insert to code</b> then select the <b>Insert Credentials</b> link to have a credentials dictionary added to the notebook.  \n",
    "\n",
    "\n",
    "\n",
    "Connecting to Cloud Object Storage requires the following information which are provided by the credentials dictionary inserted. \n",
    "\n",
    "    IBM_API_KEY\n",
    "    IAM_SERVICE_ID\n",
    "    ENDPOINT\n",
    "    IBM_AUTH_ENDPOINT\n",
    "\n",
    "We will upload the results file to the same bucket that contains the cancer_data.csv file.  \n",
    "   \n",
    "The @hidden_cell directive tells DSX not to export credentials when sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert object storage credentials below\n",
    "# Make sure the name that is used is credentials. If credentials_1 is shown, please change to credentials. \n",
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "credentials = {\n",
    "    'IBM_API_KEY_ID': '54eK5hQft9uVYbrpsxfWs4Tdgw8sbc8weZndKdj8iwhj',\n",
    "    'IAM_SERVICE_ID': 'iam-ServiceId-e54eecd9-137d-44ca-b6d3-87b14b0ad21a',\n",
    "    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'IBM_AUTH_ENDPOINT': 'https://iam.ng.bluemix.net/oidc/token',\n",
    "    'BUCKET': 'mlproject-donotdelete-pr-rwqegunlryj5yq',\n",
    "    'FILE': 'cancer_data.csv'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesToWrite= newPreds.select(\"id\",  \"predCategory\",\"diagnosis\").toPandas().to_csv(path_or_buf=\"cancer_results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config\n",
    "service_endpoint=credentials['ENDPOINT']\n",
    "auth_endpoint=credentials['IBM_AUTH_ENDPOINT']\n",
    "cos = ibm_boto3.client('s3',\n",
    "                      ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n",
    "                      ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n",
    "                      ibm_auth_endpoint=auth_endpoint,\n",
    "                      config=Config(signature_version='oauth'),\n",
    "                      endpoint_url=service_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"cancer_results.csv\"\n",
    "bucket = credentials['BUCKET']\n",
    "cos.upload_file(Filename=file, Bucket=bucket, Key=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look in our Cloud Object Storage on the IBM Cloud for the results of our predictions. Within the bucket specified for this project (found in credentials inserted 4 cells above), we can find a .csv file that contains the results of our models prediction and we are able to state whether a tumor is benign or malignant with confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark 2.1",
   "language": "python",
   "name": "python3-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
